# ETL Project with Flask, Neo4j, PostgreSQL, and Docker

## ğŸ“Œ Project Overview
This project implements an **ETL (Extract, Transform, Load) pipeline** using:
- **Flask API** to extract data from **Neo4j**
- Transformation logic for **movies** or **programming languages**
- Load the transformed data into **PostgreSQL**
- Export final data to a **CSV file**
- **Docker Compose** to manage services

## ğŸ“¦ Technologies Used
- **Python (Flask)** - API development
- **Neo4j** - Graph database (source data)
- **PostgreSQL** - Relational database (destination)
- **Docker & Docker Compose** - Containerization

---

## ğŸš€ How to Run the Project
### 1ï¸âƒ£ Clone the Repository
```sh
git clone https://github.com/YOUR_GITHUB_USERNAME/etl_project.git
cd etl_project
```

### 2ï¸âƒ£ Start the Docker Containers
```sh
docker-compose up --build -d
```
This will:
- Start **Neo4j**, **PostgreSQL**, and the **Flask API**
- Create necessary networks and volumes

### 3ï¸âƒ£ Check Running Containers
```sh
docker ps
```
Ensure that all required services are **Up**.

---

## ğŸ”„ Running the ETL Pipeline

### **Extract Data**
Get raw data from Neo4j:
```sh
curl http://localhost:5000/api/extract
```

### **Transform Data**
Apply transformations:
```sh
curl http://localhost:5000/api/transform
```

### **Load Data into PostgreSQL**
Run the ETL load process:
```sh
curl http://localhost:5000/api/load
```

### **Export Data to CSV**
```sh
curl http://localhost:5000/api/export
```
The CSV file (`recap.csv`) will be saved in the **mounted volume**.

---

## ğŸ›  Stopping and Restarting

### **Stop Containers**
```sh
docker-compose down
```

### **Restart Containers**
```sh
docker-compose up -d
```

---

## ğŸ“‚ Project Structure
```
ğŸ“‚ etl_project/
â”œâ”€â”€ ğŸ“„ Dockerfile
â”œâ”€â”€ ğŸ“„ docker-compose.yml
â”œâ”€â”€ ğŸ“‚ app/
â”‚   â”œâ”€â”€ ğŸ“„ app.py        # Flask API
â”‚   â”œâ”€â”€ ğŸ“„ extract.py    # Extract logic (Neo4j)
â”‚   â”œâ”€â”€ ğŸ“„ transform.py  # Transform logic
â”‚   â”œâ”€â”€ ğŸ“„ load.py       # Load logic (PostgreSQL)
â”‚   â”œâ”€â”€ ğŸ“„ export.py     # Export to CSV
â”œâ”€â”€ ğŸ“„ requirements.txt  # Dependencies
â””â”€â”€ ğŸ“„ README.md         # Project Documentation
```

---

## ğŸ“ Notes
- **Ensure Docker is installed** before running the project.
- The dataset is chosen based on **your student ID (odd/even).**
- Modify `.env` if needed for **database credentials**.

âœ… **Now your ETL project is ready to run!** ğŸš€

